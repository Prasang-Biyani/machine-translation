{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prasang-Biyani/machine-translation/blob/main/Seq2SeqImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwPL0hIlGKoA"
      },
      "source": [
        "# <font color='red'>**Sequence to sequence implementation**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyfZo8fmLOec"
      },
      "source": [
        "## Task -1: Simple Encoder and Decoder\n",
        "Implement simple Encoder-Decoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvNSZXNkkOkO"
      },
      "source": [
        "1. Download the **Italian** to **English** translation dataset from <a href=\"http://www.manythings.org/anki/ita-eng.zip\">here</a>\n",
        "\n",
        "2. You will find **ita.txt** file in that ZIP, \n",
        "you can read that data using python and preprocess that data this way only: \n",
        "<img src='https://i.imgur.com/z0j79Jf.png'>    \n",
        "    \n",
        "3. You have to implement a simple Encoder and Decoder architecture  \n",
        "\n",
        "4. Use BLEU score as metric to evaluate your model. You can use any loss function you need.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCd7hBs1DBlI",
        "outputId": "a00e70c0-6a54-4994-ccc8-f8c9d082df65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Seq2Seq\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr9QMk05DXWH"
      },
      "outputs": [],
      "source": [
        "# !wget http://www.manythings.org/anki/ita-eng.zip\n",
        "# !unzip *.zip\n",
        "# !wget https://www.dropbox.com/s/ddkmtqz01jc024u/glove.6B.100d.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k_AlAuKJqVA"
      },
      "source": [
        "<font color='green'>**Load the data**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU80Ao-AGaob"
      },
      "outputs": [],
      "source": [
        "with open(\"ita.txt\", 'r') as f:\n",
        "  text = f.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmGWTdRmKRph"
      },
      "source": [
        "<font color='green'>**Preprocess data**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QqElB_nKZos"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# decoder_ip = []\n",
        "# decoder_op = []\n",
        "# encoder = []\n",
        "\n",
        "# english = []\n",
        "# italian = []\n",
        "\n",
        "# for txt in text:\n",
        "#   char_1, char_2 = txt.split('\\t')[:2]\n",
        "#   english.append(char_1)\n",
        "#   italian.append(char_2)\n",
        "\n",
        "\n",
        "# decoder_ip = [\"<start> \" + eng for eng in english]\n",
        "# encoder = [\"<start> \" + ita + \" <end>\" for ita in italian]\n",
        "# decoder_op = [eng + \" <end>\" for eng in english]\n",
        "\n",
        "\n",
        "# df = pd.DataFrame(data={\n",
        "#     \"italian\": italian,\n",
        "#     \"english\": english,\n",
        "#     \"encoder\": encoder,\n",
        "#     \"decoder_ip\": decoder_ip,\n",
        "#     \"decoder_op\": decoder_op\n",
        "# })\n",
        "\n",
        "# df.to_csv('preprocessed.csv', index=False)\n",
        "df = pd.read_csv('preprocessed.csv')\n",
        "max_words_encoder = max([len(sentence.split()) for sentence in df['encoder'].values])\n",
        "max_words_decoder_ip = max([len(sentence.split()) for sentence in df['decoder_ip'].values])\n",
        "max_words_decoder_op = max([len(sentence.split()) for sentence in df['decoder_op'].values])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8RDrP4xKabR"
      },
      "source": [
        "## <font color='blue'>**Implement custom encoder decoder**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A45uc0JILMlV"
      },
      "source": [
        "<font color='blue'>**Encoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6U5MV7SGnvv",
        "outputId": "591a0670-840a-438f-ac6c-9948d95cfbfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 30.7 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 39.0 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 194 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 256 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 276 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 307 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 337 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 358 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 368 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 389 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 440 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 450 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 471 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 481 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 501 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 512 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 522 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 532 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 542 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 552 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 563 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 573 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 583 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 604 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 614 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 624 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 634 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 645 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 655 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 675 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 686 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 696 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 706 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 716 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 727 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 737 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 747 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 757 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 768 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 778 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 788 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 798 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 808 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 819 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 829 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 849 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 860 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 870 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 880 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 890 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 901 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 911 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 921 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 931 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 942 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 952 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 962 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 972 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 983 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 993 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.0 MB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1 MB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 14.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "%load_ext tensorboard\n",
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C4yaVywfr10"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test = train_test_split(df, test_size=0.2, random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDHJSUNJck1a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLTXiQjumdck"
      },
      "outputs": [],
      "source": [
        "encoder_tokenizer = Tokenizer(filters='\"#$%&()*+,-/:;=?@[\\\\]^`{|}~\\t\\n')\n",
        "encoder_tokenizer.fit_on_texts(X_train['encoder'].values)\n",
        "\n",
        "vocab_size_encoder = len(encoder_tokenizer.word_index) + 1\n",
        "\n",
        "train_encoder_sequences = encoder_tokenizer.texts_to_sequences(X_train['encoder'].values)\n",
        "test_encoder_sequences = encoder_tokenizer.texts_to_sequences(X_test['encoder'].values)\n",
        "\n",
        "# padding sequences\n",
        "train_encoder_padded = pad_sequences(train_encoder_sequences, padding='post', maxlen=max_words_encoder)\n",
        "test_encoder_padded = pad_sequences(test_encoder_sequences, padding='post', maxlen=max_words_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ykd99KqlzZ5"
      },
      "outputs": [],
      "source": [
        "decoder_ip_tokenizer = Tokenizer(filters='\"#$%&()*+,-/:;=?@[\\\\]^`{|}~\\t\\n')\n",
        "decoder_ip_tokenizer.fit_on_texts(X_train['decoder_ip'].values)\n",
        "\n",
        "vocab_size_decoder_ip = len(decoder_ip_tokenizer.word_index) + 1\n",
        "\n",
        "train_decoder_ip_sequences = decoder_ip_tokenizer.texts_to_sequences(X_train['decoder_ip'].values)\n",
        "test_decoder_ip_sequences = decoder_ip_tokenizer.texts_to_sequences(X_test['decoder_ip'].values)\n",
        "\n",
        "#padding sequences\n",
        "train_decoder_padded_ip= pad_sequences(train_decoder_ip_sequences, padding='post', maxlen=max_words_decoder_ip)\n",
        "test_decoder_padded_ip = pad_sequences(test_decoder_ip_sequences, padding='post', maxlen=max_words_decoder_ip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi4RUPmg5IaQ"
      },
      "outputs": [],
      "source": [
        "decoder_op_tokenizer = Tokenizer(filters='\"#$%&()*+,-/:;=?@[\\\\]^`{|}~\\t\\n')\n",
        "decoder_op_tokenizer.fit_on_texts(X_train['decoder_op'].values)\n",
        "\n",
        "vocab_size_decoder_op = len(decoder_op_tokenizer.word_index) + 1\n",
        "\n",
        "train_decoder_op_sequences = decoder_op_tokenizer.texts_to_sequences(X_train['decoder_op'].values)\n",
        "test_decoder_op_sequences = decoder_op_tokenizer.texts_to_sequences(X_test['decoder_op'].values)\n",
        "\n",
        "#padding sequences\n",
        "train_decoder_padded_op = pad_sequences(train_decoder_op_sequences, padding='post', maxlen=max_words_decoder_op)\n",
        "test_decoder_padded_op = pad_sequences(test_decoder_op_sequences, padding='post', maxlen=max_words_decoder_op)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h96rplW_KLJp"
      },
      "outputs": [],
      "source": [
        "embeddings_index = dict()\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size_decoder_ip, 100))\n",
        "for word, i in decoder_ip_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIkkI7RROYZ_",
        "outputId": "4bb67236-a8bb-46ed-a4f1-c499f9b3445d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(19464, 100)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlyFvIFwVnWa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten, concatenate, LSTM, Dropout, ConvLSTM1D, Conv1D\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cex2XfCLOew"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns encoder-outputs, encoder_final_state_h, encoder_final_state_c\n",
        "    '''\n",
        "\n",
        "    def __init__(self, inp_vocab_size, embedding_size, lstm_size, input_length):\n",
        "\n",
        "        super().__init__()\n",
        "        #Initialize Embedding layer\n",
        "        #Intialize Encoder LSTM layer\n",
        "        self.vocab_size = inp_vocab_size\n",
        "        self.embedding_dim = embedding_size\n",
        "        self.input_length = input_length\n",
        "        self.lstm_units = lstm_size\n",
        "        self.lstm_output = 0\n",
        "        self.lstm_state_h = 0\n",
        "        self.lstm_state_c = 0\n",
        "\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_encoder\", trainable=True)\n",
        "        self.lstm_encoder = LSTM(self.lstm_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "\n",
        "\n",
        "    def call(self, input_sequence, states=[0, 0]):\n",
        "        '''\n",
        "          This function takes a sequence input and the initial states of the encoder.\n",
        "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
        "          returns -- encoder_output, last time step's hidden and cell state\n",
        "        '''\n",
        "        input_embeddings = self.embedding(input_sequence)\n",
        "        self.lstm_output, self.lstm_state_h, self.lstm_state_c = self.lstm_encoder(input_embeddings)\n",
        "        return self.lstm_output, self.lstm_state_h, self.lstm_state_c    \n",
        "\n",
        "    \n",
        "    def initialize_states(self, batch_size):\n",
        "      '''\n",
        "      Given a batch size it will return intial hidden state and intial cell state.\n",
        "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
        "      '''\n",
        "      self.lstm_output = np.zeros((batch_size, self.lstm_units))\n",
        "      self.lstm_state_h = np.zeros((batch_size, self.lstm_units))\n",
        "      self.lstm_state_c = np.zeros((batch_size, self.lstm_units))\n",
        "      return self.lstm_state_h, self.lstm_state_c\n",
        "      \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1ES1-sJLOe4"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    '''\n",
        "    Decoder model -- That takes a input sequence and returns output sequence\n",
        "    '''\n",
        "\n",
        "    def __init__(self, out_vocab_size, embedding_size, lstm_size, input_length):\n",
        "\n",
        "        super().__init__()\n",
        "        #Initialize Embedding layer\n",
        "        #Intialize Decoder LSTM layer\n",
        "        self.vocab_size = out_vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.input_length = input_length\n",
        "        if self.embedding_size == 100:\n",
        "          self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_decoder\", trainable=False, weights=[embedding_matrix])\n",
        "        else:\n",
        "          self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_decoder\", trainable=True)\n",
        "\n",
        "        self.lstm_decoder = LSTM(self.lstm_size, return_sequences=True, return_state=True, name=\"Decoder_LSTM\")        \n",
        "\n",
        "\n",
        "    def call(self, input_sequence, initial_states):\n",
        "        '''\n",
        "          This function takes a sequence input and the initial states of the encoder.\n",
        "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to decoder_lstm\n",
        "        \n",
        "          returns -- decoder_output,decoder_final_state_h,decoder_final_state_c\n",
        "        '''\n",
        "        target_embedding = self.embedding(input_sequence)\n",
        "        decoder_output, decoder_final_state_h, decoder_final_state_c  = self.lstm_decoder(target_embedding, initial_state=initial_states)\n",
        "        return decoder_output, decoder_final_state_h, decoder_final_state_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXrIj4scLOe_"
      },
      "outputs": [],
      "source": [
        "class Encoder_Decoder(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, **params):\n",
        "        super().__init__()\n",
        "        #Create encoder object\n",
        "        #Create decoder object\n",
        "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
        "\n",
        "        self.batch_size = params['BATCH_SIZE']\n",
        "        self.embedding_size_encoder = params['embedding_size_encoder']\n",
        "        self.embedding_size_decoder = params['embedding_size_decoder']\n",
        "        self.encoder = Encoder(inp_vocab_size=vocab_size_encoder, embedding_size=self.embedding_size_encoder, lstm_size=lstm_size, input_length=max_words_encoder)\n",
        "        self.decoder = Decoder(out_vocab_size=vocab_size_decoder_ip, embedding_size=self.embedding_size_decoder, lstm_size=lstm_size, input_length=max_words_decoder_ip)\n",
        "        self.dense_layer = Dense(vocab_size_decoder_op, activation='softmax')\n",
        "        self.initial_state = self.encoder.initialize_states(self.batch_size)\n",
        "    \n",
        "    def call(self, data):\n",
        "        '''\n",
        "        A. Pass the input sequence to Encoder layer -- Return encoder_output,encoder_final_state_h,encoder_final_state_c\n",
        "        B. Pass the target sequence to Decoder layer with intial states as encoder_final_state_h,encoder_final_state_C\n",
        "        C. Pass the decoder_outputs into Dense layer \n",
        "        \n",
        "        Return decoder_outputs\n",
        "        '''\n",
        "        params = dict()\n",
        "        params['input_sequence'], params['output_sequence'] = data[0], data[1]\n",
        "        encoder_output, encoder_h, encoder_c = self.encoder(params['input_sequence'], [self.initial_state])\n",
        "        decoder_output, decoder_h, decoder_c = self.decoder(params['output_sequence'], [encoder_h, encoder_c])\n",
        "        dense_output = self.dense_layer(decoder_output)\n",
        "        # dense_output = self.time_distrbuted(dense_output)\n",
        "        return dense_output        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtW8yxIZQS1s"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "logdir = os.path.join(\"vanilla_encoder_decoder_tensorboard_logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "lstm_size=256\n",
        "\n",
        "# Create an object of encoder_decoder Model class, \n",
        "# Compile the model and fit the model\n",
        "\n",
        "model = Encoder_Decoder(BATCH_SIZE=32, embedding_size_encoder=50, embedding_size_decoder=100)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcL61dJXLOfB",
        "outputId": "5d767c17-06ff-4072-bd58-0bf909d96569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "552/552 [==============================] - 136s 236ms/step - loss: 0.3404 - val_loss: 0.2947\n",
            "Epoch 2/10\n",
            "552/552 [==============================] - 126s 229ms/step - loss: 0.2699 - val_loss: 0.2413\n",
            "Epoch 3/10\n",
            "552/552 [==============================] - 126s 229ms/step - loss: 0.2197 - val_loss: 0.1991\n",
            "Epoch 4/10\n",
            "552/552 [==============================] - 126s 229ms/step - loss: 0.1829 - val_loss: 0.1713\n",
            "Epoch 5/10\n",
            "552/552 [==============================] - 126s 229ms/step - loss: 0.1556 - val_loss: 0.1486\n",
            "Epoch 6/10\n",
            "552/552 [==============================] - 126s 229ms/step - loss: 0.1333 - val_loss: 0.1304\n",
            "Epoch 7/10\n",
            "552/552 [==============================] - 126s 229ms/step - loss: 0.1151 - val_loss: 0.1165\n",
            "Epoch 8/10\n",
            "552/552 [==============================] - 126s 229ms/step - loss: 0.1001 - val_loss: 0.1060\n",
            "Epoch 9/10\n",
            "552/552 [==============================] - 126s 229ms/step - loss: 0.0877 - val_loss: 0.0959\n",
            "Epoch 10/10\n",
            "552/552 [==============================] - 126s 229ms/step - loss: 0.0775 - val_loss: 0.0883\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6cc9f8cd50>"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!rm -rf vanilla_encoder_decoder_tensorboard_logs\n",
        "model.fit([train_encoder_padded, train_decoder_padded_ip], train_decoder_padded_op, epochs=10, batch_size=512, \n",
        "          validation_data=([test_encoder_padded, test_decoder_padded_ip], test_decoder_padded_op), callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot tensorboard graphs"
      ],
      "metadata": {
        "id": "D7ZzYKklsxvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkARSlZgLOfE"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def predict(input_sentence):\n",
        "\n",
        "  '''\n",
        "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
        "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
        "  C. Initialize index of <start> as input to decoder. and encoder final states as input_states to decoder\n",
        "  D. till we reach max_length of decoder or till the model predicted word <end>:\n",
        "         predicted_out,state_h,state_c=model.layers[1](dec_input,states)\n",
        "         pass the predicted_out to the dense layer\n",
        "         update the states=[state_h,state_c]\n",
        "         And get the index of the word with maximum probability of the dense layer output, using the tokenizer(word index) get the word and then store it in a string.\n",
        "         Update the input_to_decoder with current predictions\n",
        "  F. Return the predicted sentence\n",
        "  '''\n",
        "  encoder_outputs, encoder_h, encoder_c = model.layers[0](np.expand_dims(input_sentence, 0))\n",
        "  states = [encoder_h, encoder_c]\n",
        "  curr_vector = np.full((1, 1), encoder_tokenizer.word_index['<start>'])\n",
        "  prediction = []\n",
        "  for i in range(test_decoder_padded_ip.shape[1]):\n",
        "    curr_embedding = model.layers[1].embedding(curr_vector)\n",
        "    decoder_output, decoder_h, decoder_c = model.layers[1].lstm_decoder(curr_embedding, states)\n",
        "    states = [decoder_h, decoder_c]\n",
        "    output = model.layers[2](decoder_output)\n",
        "    curr_vector = np.reshape(np.argmax(output), (1, 1))\n",
        "    if curr_vector[0] == decoder_op_tokenizer.word_index['<end>']:\n",
        "      break\n",
        "    prediction.append(curr_vector[0][0])\n",
        "  predicted_word = [decoder_op_tokenizer.index_word[pred] for pred in prediction]\n",
        "  return predicted_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsK3_RL4za5i"
      },
      "outputs": [],
      "source": [
        "np.random.seed(101)\n",
        "indexes = np.random.permutation(1000)\n",
        "predicted_thousand_sentences = [\" \".join(predict(test_encoder_padded[index])) for index in indexes]\n",
        "original_thousand_sentences = [X_test['english'].iloc[index] for index in indexes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJpyLZwhjDyX",
        "outputId": "44353c41-f793-4c14-fc5d-0d21f5cb488d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(\"We're unusual.\", \"we're unusual.\")"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "original_thousand_sentences[0], predicted_thousand_sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "996pFO8BLOfG",
        "outputId": "4f670302-73c4-4eb7-8f62-753dd3489b0c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8293736751504862\n"
          ]
        }
      ],
      "source": [
        "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
        "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
        "\n",
        "import nltk.translate.bleu_score as bleu\n",
        "avg_bleu = sum([bleu.sentence_bleu(reference, translated) for reference, translated in zip(original_thousand_sentences, predicted_thousand_sentences)]) / 1000\n",
        "print(avg_bleu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxWFDxZXLOfJ"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZhX3K9GLOfJ"
      },
      "source": [
        "## Task -2: Including Attention mechanisum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3d7GeBMGbsJ"
      },
      "source": [
        "1. Use the preprocessed data from Task-1\n",
        "\n",
        "2. You have to implement an Encoder and Decoder architecture with  \n",
        "attention as discussed in the reference notebook.\n",
        "\n",
        "    * Encoder   - with 1 layer LSTM <br>\n",
        "    * Decoder   - with 1 layer LSTM<br>\n",
        "    * attention \n",
        "3. In Global attention, we have 3 types of scoring functions(as discussed in the reference notebook).\n",
        " As a part of this assignment **you need to create 3 models for each scoring function**\n",
        "<img src='https://i.imgur.com/iD2jZo3.png'>\n",
        "\n",
        "    * In model 1 you need to implemnt \"dot\" score function\n",
        "    * In model 2 you need to implemnt \"general\" score function\n",
        "    * In model 3 you need to implemnt \"concat\" score function.<br>\n",
        "    \n",
        " **Please do add the markdown titles for each model so that we can have a better look at the code and verify.**\n",
        "4. It is mandatory to train the model with simple model.fit() only, Donot train the model with custom GradientTape()\n",
        "\n",
        "5. Using attention weights, you can plot the attention plots, \n",
        "please plot those for 2-3 examples. You can check about those in <a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention#translate\">this</a>\n",
        "\n",
        "6. The attention layer has to be written by yourself only. \n",
        "The main objective of this assignment is to read and implement a paper on yourself so please do it yourself.  \n",
        "\n",
        "7. Please implement the class **onestepdecoder** as mentioned in the assignment instructions.\n",
        "\n",
        "8. You can use any tf.Keras highlevel API's to build and train the models. \n",
        " Check the reference notebook for better understanding.\n",
        "\n",
        "9. Use BLEU score as metric to evaluate your model. You can use any loss function you need.\n",
        "\n",
        "10. You have to use Tensorboard to plot the Graph, Scores and histograms of gradients. \n",
        "\n",
        "11. Resources:\n",
        "    a. Check the reference notebook\n",
        "    b. <a href=\"https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\">Resource 1</a>\n",
        "    c. <a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\">Resource 2</a>\n",
        "    d. <a href=\"https://stackoverflow.com/questions/44238154/what-is-the-difference-between-luong-attention-and-bahdanau-attention#:~:text=Luong%20attention%20used%20top%20hidden,hidden%20state%20at%20time%20t.\">Resource 3</a>\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU4KIsGxLOfK"
      },
      "source": [
        "### <font color='blue'>**Implement custom encoder decoder and attention layers**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMm3ADQDLOfK"
      },
      "source": [
        "<font color='blue'>**Encoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx_5NA24KzRp"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns output sequence\n",
        "    '''\n",
        "\n",
        "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
        "\n",
        "        #Initialize Embedding layer\n",
        "        #Intialize Encoder LSTM layer\n",
        "        super().__init__()\n",
        "        self.vocab_size = inp_vocab_size\n",
        "        self.embedding_dim = embedding_size\n",
        "        self.lstm_units = lstm_size\n",
        "        self.input_length = input_length\n",
        "        self.lstm_output = 0\n",
        "        self.lstm_state_h = 0\n",
        "        self.lstm_state_c = 0\n",
        "\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length, trainable=True)\n",
        "        self.lstm = LSTM(self.lstm_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "\n",
        "\n",
        "    def call(self, input_sequence, states=[0, 0]):\n",
        "\n",
        "      '''\n",
        "          This function takes a sequence input and the initial states of the encoder.\n",
        "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
        "          returns -- All encoder_outputs, last time steps hidden and cell state\n",
        "      '''\n",
        "      input_embeddings = self.embedding(input_sequence)\n",
        "      self.lstm_output, self.lstm_state_h, self.lstm_state_c = self.lstm(input_embeddings)\n",
        "      return self.lstm_output, self.lstm_state_h, self.lstm_state_c   \n",
        "    \n",
        "    def initialize_states(self, batch_size):\n",
        "      '''\n",
        "      Given a batch size it will return intial hidden state and intial cell state.\n",
        "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
        "      '''\n",
        "      self.lstm_output = np.zeros((batch_size, self.lstm_units))\n",
        "      self.lstm_state_h = np.zeros((batch_size, self.lstm_units))\n",
        "      self.lstm_state_c = np.zeros((batch_size, self.lstm_units))\n",
        "\n",
        "      return self.lstm_state_h, self.lstm_state_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXn278lhLYRM"
      },
      "source": [
        "<font color='blue'>**Attention**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab5SNdPZLlur"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.python.ops.numpy_ops import np_config\n",
        "# np_config.enable_numpy_behavior()\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "  '''\n",
        "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
        "  '''\n",
        "  def __init__(self, scoring_function, att_units):\n",
        "\n",
        "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.att_units = att_units\n",
        "    self.scoring_function = scoring_function\n",
        "    self.context = None\n",
        "    self.attention_weights = None\n",
        "\n",
        "    if self.scoring_function == 'dot':\n",
        "      # Intialize variables needed for Dot score function here\n",
        "      self.V = tf.keras.layers.Dense(1)\n",
        "    if scoring_function == 'general':\n",
        "      # Intialize variables needed for General score function here\n",
        "      self.W = tf.keras.layers.Dense(self.att_units)\n",
        "      self.V = tf.keras.layers.Dense(1)\n",
        "    elif scoring_function == 'concat':\n",
        "      # Intialize variables needed for Concat score function here\n",
        "      self.W1 = tf.keras.layers.Dense(self.att_units)\n",
        "      self.W2 = tf.keras.layers.Dense(self.att_units)\n",
        "      self.V = tf.keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, decoder_hidden_state, encoder_output):\n",
        "    '''\n",
        "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
        "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
        "        Multiply the score function with your encoder_outputs to get the context vector.\n",
        "        Function returns context vector and attention weights(softmax - scores)\n",
        "    '''\n",
        "    \n",
        "    if self.scoring_function == 'dot':\n",
        "        # Implement Dot score function here\n",
        "        score = []\n",
        "        # expanding decoder_hidden_state at 2nd axis\n",
        "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 1)\n",
        "        # print(tf.keras.layers.Dot(axes=(1, 2))([encoder_output, tf.transpose(decoder_hidden_state)]).shape)\n",
        "        for batch_id in range(encoder_output.shape[0]):\n",
        "          # score.append(np.dot(encoder_output[batch_id, :,  :], decoder_hidden_state[batch_id, :, :].T))\n",
        "          score.append(tf.tensordot(encoder_output[batch_id, :, :], tf.transpose(decoder_hidden_state[batch_id, :, :]), axes=1))\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        self.attention_weights = self.V(tf.convert_to_tensor(score))\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        self.attention_weights = tf.nn.softmax(self.attention_weights, axis=1)\n",
        "        # # context_vector shape == (batch_size, att_units)\n",
        "        self.context_vector = self.attention_weights * encoder_output\n",
        "        self.context_vector = tf.reduce_sum(self.context_vector, axis=1)\n",
        "\n",
        "        return self.context_vector, self.attention_weights\n",
        "\n",
        "    elif self.scoring_function == 'general':\n",
        "        # Implement General score function here\n",
        "        # expanding decoder_hidden_state at 2nd axis\n",
        "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 1)\n",
        "        score = self.V(self.W(decoder_hidden_state) * encoder_output)\n",
        "        self.attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        self.context_vector = self.attention_weights * encoder_output\n",
        "        self.context_vector = tf.reduce_sum(self.context_vector, axis=1)\n",
        "\n",
        "        return self.context_vector, self.attention_weights\n",
        "\n",
        "    # https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/#step-7-attention-mechanism-class\n",
        "    elif self.scoring_function == 'concat':\n",
        "      # Implement General score function here\n",
        "      decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 1)\n",
        "      # score shape == (batch_size, max_length, 1)\n",
        "      score = self.V(tf.nn.tanh(\n",
        "        self.W1(decoder_hidden_state) + self.W2(encoder_output)))\n",
        "      # attention_weights shape == (batch_size, max_length, 1)\n",
        "      self.attention_weights = tf.nn.softmax(score, axis=1)\n",
        "      # context_vector shape after sum == (batch_size, hidden_size)\n",
        "      self.context_vector = self.attention_weights * encoder_output\n",
        "      self.context_vector = tf.reduce_sum(self.context_vector, axis=1)\n",
        "\n",
        "      return self.context_vector, self.attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-FNEbfL2DN"
      },
      "source": [
        "<font color='blue'>**OneStepDecoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc8m7lmOL097"
      },
      "outputs": [],
      "source": [
        "class One_Step_Decoder(tf.keras.Model):\n",
        "  def __init__(self, tar_vocab_size, embedding_dim, input_length, dec_units, score_fun, att_units):\n",
        "\n",
        "      super(One_Step_Decoder, self).__init__()\n",
        "      self.att_units = att_units\n",
        "      self.scoring_function = score_fun\n",
        "      self.vocab_size = tar_vocab_size\n",
        "      self.embedding_dim = embedding_dim\n",
        "      self.input_length = input_length\n",
        "      self.dec_units = dec_units\n",
        "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
        "      if self.embedding_dim == 100:\n",
        "          self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_decoder\", trainable=False, weights=[embedding_matrix])\n",
        "      else:\n",
        "          self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_decoder\", trainable=True)\n",
        "      # self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length, mask_zero=True)\n",
        "      self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True)\n",
        "      self.attention = Attention(self.scoring_function, self.att_units)\n",
        "      # self.dense_1 = Dense(self.att_units)\n",
        "      self.dense = Dense(self.vocab_size)\n",
        "\n",
        "  def call(self, input_to_decoder, encoder_output, state_h, state_c):\n",
        "    '''\n",
        "        One step decoder mechanisim step by step:\n",
        "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
        "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
        "      C. Concat the context vector with the step A output\n",
        "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
        "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
        "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
        "    '''\n",
        "    target_embedding = self.embedding(input_to_decoder)\n",
        "    context_vector, attention_weights = self.attention(state_h, encoder_output)\n",
        "    concatenated = tf.concat([tf.expand_dims(context_vector, 1), target_embedding], axis=-1)\n",
        "    decoder_output, decoder_h, decoder_c = self.lstm(concatenated)\n",
        "    dense = self.dense(decoder_output)\n",
        "    dense = tf.reshape(dense, (-1, dense.shape[2]))\n",
        "    # dense_2 = self.dense_2(dense_1)\n",
        "    # dense = tf.squeeze(dense)\n",
        "    return dense, decoder_h, decoder_c, attention_weights, context_vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHrurjUMGAi"
      },
      "source": [
        "<font color='blue'>**Decoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV-x31rj6Hc4"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, out_vocab_size, embedding_dim, input_length, dec_units, score_fun, att_units):\n",
        "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
        "      super(Decoder, self).__init__()\n",
        "      self.vocab_size = out_vocab_size\n",
        "      self.embedding_dim = embedding_dim\n",
        "      self.input_length = input_length\n",
        "      self.dec_units = dec_units\n",
        "      self.score_fun = score_fun\n",
        "      self.att_units = att_units\n",
        "\n",
        "      self.one_step_decoder = One_Step_Decoder(self.vocab_size, self.embedding_dim, self.input_length,\n",
        "                                               self.dec_units, self.score_fun, self.att_units)     \n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, input_to_decoder, encoder_output, decoder_hidden_state, decoder_cell_state):\n",
        "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
        "        #Create a tensor array as shown in the reference notebook\n",
        "        \n",
        "        #Iterate till the length of the decoder input\n",
        "            # Call onestepdecoder for each token in decoder_input\n",
        "            # Store the output in tensorarray\n",
        "        # Return the tensor array\n",
        "        all_outputs = tf.TensorArray(tf.float32, size=input_to_decoder.shape[1], name='output_arrays')\n",
        "        for time_step in range(input_to_decoder.shape[1]):\n",
        "          output, decoder_hidden_state, _, _, _= self.one_step_decoder(input_to_decoder[:, time_step : time_step + 1], encoder_output, decoder_hidden_state, decoder_cell_state)\n",
        "          # output, decoder_hidden_state, _, _, _= self.one_step_decoder(input_to_decoder, encoder_output, decoder_hidden_state, decoder_cell_state)\n",
        "          all_outputs = all_outputs.write(time_step, output)\n",
        "        all_outputs = tf.transpose(all_outputs.stack(), [1, 0, 2])\n",
        "        # print(all_outputs.shape)\n",
        "        return all_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC1T1EOoMTqC"
      },
      "source": [
        "<font color='blue'>**Encoder Decoder model**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfqBIe20MT3D"
      },
      "outputs": [],
      "source": [
        "class encoder_decoder(tf.keras.Model):\n",
        "  def __init__(self, params):\n",
        "    super(encoder_decoder, self).__init__()\n",
        "    #Intialize objects from encoder decoder\n",
        "    self.input_vocab_size = params['input_vocab_size']\n",
        "    self.encoder_embedding_dim = params['encoder_embedding_dim']\n",
        "    self.lstm_size = params['lstm_size']\n",
        "    self.encoder_input_length = params['encoder_input_length']\n",
        "    self.scoring_function = params['scoring_function']\n",
        "    self.attention_units = params['attention_units']\n",
        "    self.output_vocab_size = params['output_vocab_size']\n",
        "    self.decoder_embedding_dim = params['decoder_embedding_dim']\n",
        "    self.decoder_input_length = params['decoder_input_length']\n",
        "\n",
        "    self.encoder = Encoder(inp_vocab_size=self.input_vocab_size, embedding_size=self.encoder_embedding_dim, \n",
        "                           lstm_size=self.lstm_size, input_length=self.encoder_input_length)\n",
        "    \n",
        "    self.decoder = Decoder(out_vocab_size=self.output_vocab_size, embedding_dim=self.decoder_embedding_dim, \n",
        "                           input_length=self.decoder_input_length,\n",
        "                           dec_units=self.lstm_size, score_fun=self.scoring_function, att_units=self.attention_units)\n",
        "  @tf.function\n",
        "  def call(self, data):\n",
        "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
        "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
        "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
        "    # return the decoder output\n",
        "    encoder_ip, decoder_ip = data[0], data[1]\n",
        "    encoder_output, encoder_h, encoder_c = self.encoder(encoder_ip)\n",
        "    decoder_output = self.decoder(decoder_ip, encoder_output, encoder_h, encoder_c)\n",
        "    return decoder_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVRxB-FDMJWL"
      },
      "source": [
        "<font color='blue'>**Custom loss function**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY_3izrXMs8y"
      },
      "outputs": [],
      "source": [
        "#https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Custom loss function that will not consider the loss for padded zeros.\n",
        "    why are we using this, can't we use simple sparse categorical crossentropy?\n",
        "    Yes, you can use simple sparse categorical crossentropy as loss like we did in task-1. But in this loss function we are ignoring the loss\n",
        "    for the padded zeros. i.e when the input is zero then we donot need to worry what the output is. This padded zeros are added from our end\n",
        "    during preprocessing to make equal length for all the sentences.\n",
        "\n",
        "    \"\"\"\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QlbWAqNNlqe"
      },
      "source": [
        "<font color='blue'>**Training**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqtZUQF2NuZE"
      },
      "source": [
        "Implement dot function here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7h7k0ig044V"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "logdir = os.path.join(\"attention_encoder_decoder_tensorboard_logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgyWwZWeMxGQ"
      },
      "outputs": [],
      "source": [
        "# Implement teacher forcing while training your model. You can do it two ways.\n",
        "# Prepare your data, encoder_input,decoder_input and decoder_output\n",
        "# if decoder input is \n",
        "# <start> Hi how are you\n",
        "# decoder output should be\n",
        "# Hi How are you <end>\n",
        "# i.e when you have send <start>-- decoder predicted Hi, 'Hi' decoder predicted 'How' .. e.t.c\n",
        "\n",
        "# or\n",
        " \n",
        "# model.fit([train_ita,train_eng],train_eng[:,1:]..)\n",
        "# Note: If you follow this approach some grader functions might return false and this is fine.\n",
        "params = dict()\n",
        "params['input_vocab_size'] = vocab_size_encoder\n",
        "params['encoder_embedding_dim'] = 100\n",
        "params['lstm_size'] = 64\n",
        "params['encoder_input_length'] = 20\n",
        "params['scoring_function'] = 'dot'\n",
        "params['attention_units'] = 64\n",
        "\n",
        "params['output_vocab_size'] = vocab_size_decoder_ip\n",
        "params['decoder_embedding_dim'] = 100\n",
        "params['decoder_input_length'] = 20\n",
        "\n",
        "model = encoder_decoder(params)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=loss_function, run_eagerly=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO6WxT9W0fAD",
        "outputId": "be3895ab-e481-4f1e-fc7a-4c5935c184a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4416/4416 [==============================] - 6885s 1s/step - loss: 0.3198 - val_loss: 0.2736\n",
            "Epoch 2/10\n",
            "4416/4416 [==============================] - 6275s 1s/step - loss: 0.2619 - val_loss: 0.2499\n",
            "Epoch 3/10\n",
            "4416/4416 [==============================] - 6261s 1s/step - loss: 0.2415 - val_loss: 0.2341\n",
            "Epoch 4/10\n",
            "4416/4416 [==============================] - 6239s 1s/step - loss: 0.2283 - val_loss: 0.2241\n",
            "Epoch 5/10\n",
            "4416/4416 [==============================] - 6185s 1s/step - loss: 0.2192 - val_loss: 0.2175\n",
            "Epoch 6/10\n",
            "4416/4416 [==============================] - 6125s 1s/step - loss: 0.2124 - val_loss: 0.2135\n",
            "Epoch 7/10\n",
            "4416/4416 [==============================] - 6127s 1s/step - loss: 0.2068 - val_loss: 0.2069\n",
            "Epoch 8/10\n",
            "4416/4416 [==============================] - 6128s 1s/step - loss: 0.2018 - val_loss: 0.2036\n",
            "Epoch 9/10\n",
            "4416/4416 [==============================] - 6085s 1s/step - loss: 0.1974 - val_loss: 0.1993\n",
            "Epoch 10/10\n",
            "4416/4416 [==============================] - 6120s 1s/step - loss: 0.1934 - val_loss: 0.1964\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5981f15a50>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!rm -rf attention_encoder_decoder_tensorboard_logs\n",
        "# validation_data=([test_encoder_padded, test_decoder_padded_ip], test_decoder_padded_op)\n",
        "model.fit([train_encoder_padded, train_decoder_padded_ip], train_decoder_padded_op, epochs=10, batch_size=64,\n",
        "          validation_data=([test_encoder_padded, test_decoder_padded_ip], test_decoder_padded_op),\n",
        "          callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dot-based attention:\n",
        "1. It is the dot product between the encoder output states and the current decoder \n",
        "state.\n",
        "2. The shape of encoder output states is (batch_size, input_length, lstm_units).\n",
        "3. The shape of the decoder's current state is (batch_size, lstm_units).\n",
        "4. A general rule of the dot product: #columns (first matrix) == #rows (second matrix).\n",
        "5. We add another dimension at axis=1 in the current decoder state to make the dot product easier.\n",
        "6. We are iterating through the #batches and storing them in a list.\n",
        "7. Finally, we calculate attention weights after passing the list on the softmax function.\n",
        "8. A context vector is calculated by the Sum Of Product (SOP) between encoder output states and attention weights."
      ],
      "metadata": {
        "id": "RI0tokmt-8dA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DpC9zlzMcXp"
      },
      "source": [
        "## <font color='blue'>**Inference**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5NhESYyMW_t"
      },
      "source": [
        "<font color='blue'>**Plot attention weights**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pkEY7SsBMtrC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def plot_attention(attention, actual_sentence, predicted_sentence):\n",
        "  #Refer: https://www.tensorflow.org/tutorials/text/nmt_with_attention#translate\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  \n",
        "  predicted_sentence.append('<end>')\n",
        "  attention = attention[:len(predicted_sentence), :len(actual_sentence)]\n",
        "  \n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + actual_sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')\n",
        "  plt.suptitle('Attention weights')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1IhdBrgQYJr"
      },
      "source": [
        "<font color='blue'>**Predict the sentence translation**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MP3kLZoPMvSu"
      },
      "outputs": [],
      "source": [
        "def predict(input_sentence, visualize_attention):\n",
        "  '''\n",
        "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
        "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
        "  C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
        "  D. till we reach max_length of decoder or till the model predicted word <end>:\n",
        "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
        "         Save the attention weights\n",
        "         And get the word using the tokenizer(word index) and then store it in a string.\n",
        "  E. Call plot_attention(#params)\n",
        "  F. Return the predicted sentence\n",
        "  '''\n",
        "  encoder_output, encoder_h, encoder_c = model.layers[0](np.expand_dims(input_sentence, 0))\n",
        "  actual_sentence = [encoder_tokenizer.index_word[token] for token in input_sentence if token != 0]\n",
        "  states = [encoder_h, encoder_c]\n",
        "  curr_vector = np.full((1, 1), encoder_tokenizer.word_index['<start>'])\n",
        "  prediction = []\n",
        "  attention_weights_lst = []\n",
        "  for i in range(test_decoder_padded_ip.shape[1]):\n",
        "    decoder_output, decoder_h, decoder_c, attention_weights, context_vector = model.layers[1].one_step_decoder(curr_vector, encoder_output, states[0], states[1])\n",
        "    attention_weights_lst.append(attention_weights)\n",
        "    states = [decoder_h, decoder_c]\n",
        "    # output = model.layers[1](decoder_output)\n",
        "    curr_vector = np.reshape(np.argmax(decoder_output), (1, 1))\n",
        "    if curr_vector[0][0] == decoder_op_tokenizer.word_index['<end>']:\n",
        "      break\n",
        "    prediction.append(curr_vector[0][0])\n",
        "  predicted_sentence = [decoder_op_tokenizer.index_word[pred] for pred in prediction]\n",
        "  # call plot_attention\n",
        "  if visualize_attention == True:\n",
        "    attention_weights_numpy = np.squeeze(np.asarray(attention_weights_lst), axis=(1, 3))\n",
        "    plot_attention(attention_weights_numpy, actual_sentence, predicted_sentence)\n",
        "  return predicted_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "9CvDbcK5EdNc",
        "outputId": "60bf004c-c50e-4992-c3a2-4e0541ac2e5d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"what's your favorite color of the station <end>\""
            ]
          },
          "execution_count": 223,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\" \".join(predict(test_encoder_padded[0], visualize_attention=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmxIVOOQPWMu"
      },
      "source": [
        "<font color='blue'>**Calculate BLEU score**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Rh9_w79M5JO"
      },
      "outputs": [],
      "source": [
        "#Compile and train your model on general scoring function.\n",
        "# Visualize few sentences randomly in Test data\n",
        "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
        "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
        "np.random.seed(101)\n",
        "indexes = np.random.permutation(1000)\n",
        "predicted_thousand_sentences = [\" \".join(predict(test_encoder_padded[index], visualize_attention=False)) for index in indexes]\n",
        "original_thousand_sentences = [X_test['english'].iloc[index] for index in indexes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQnp-XO2aquV",
        "outputId": "89f89076-7021-416e-a682-d9d5d5341765"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7859763326731088\n"
          ]
        }
      ],
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "avg_bleu = sum([bleu.sentence_bleu(reference, translated) for reference, translated in zip(original_thousand_sentences, predicted_thousand_sentences)]) / 1000\n",
        "print(avg_bleu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWg2ferDQvT3"
      },
      "source": [
        "<font color='blue'>**Repeat the same steps for General scoring function**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aelgKwYYbITB"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "logdir = os.path.join(\"general_attention_encoder_decoder_tensorboard_logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8XfSYNHa5gG"
      },
      "outputs": [],
      "source": [
        "params = dict()\n",
        "params['input_vocab_size'] = vocab_size_encoder\n",
        "params['encoder_embedding_dim'] = 100\n",
        "params['lstm_size'] = 64\n",
        "params['encoder_input_length'] = 20\n",
        "params['scoring_function'] = 'general'\n",
        "params['attention_units'] = 64\n",
        "\n",
        "params['output_vocab_size'] = vocab_size_decoder_ip\n",
        "params['decoder_embedding_dim'] = 100\n",
        "params['decoder_input_length'] = 20\n",
        "\n",
        "model = encoder_decoder(params)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=loss_function, run_eagerly=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TblNiYfKa_YG",
        "outputId": "068e6785-bea6-47df-9138-5c5f6fe55523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4416/4416 [==============================] - 2502s 536ms/step - loss: 0.3300 - val_loss: 0.2597\n",
            "Epoch 2/10\n",
            "4416/4416 [==============================] - 2294s 519ms/step - loss: 0.2217 - val_loss: 0.1935\n",
            "Epoch 3/10\n",
            "4416/4416 [==============================] - 2257s 511ms/step - loss: 0.1649 - val_loss: 0.1463\n",
            "Epoch 4/10\n",
            "4416/4416 [==============================] - 2221s 503ms/step - loss: 0.1258 - val_loss: 0.1194\n",
            "Epoch 5/10\n",
            "4416/4416 [==============================] - 2272s 514ms/step - loss: 0.1016 - val_loss: 0.1038\n",
            "Epoch 6/10\n",
            "4416/4416 [==============================] - 2277s 516ms/step - loss: 0.0862 - val_loss: 0.0937\n",
            "Epoch 7/10\n",
            "4416/4416 [==============================] - 2285s 517ms/step - loss: 0.0755 - val_loss: 0.0868\n",
            "Epoch 8/10\n",
            "4416/4416 [==============================] - 2243s 508ms/step - loss: 0.0673 - val_loss: 0.0824\n",
            "Epoch 9/10\n",
            "4416/4416 [==============================] - 2207s 500ms/step - loss: 0.0610 - val_loss: 0.0781\n",
            "Epoch 10/10\n",
            "4416/4416 [==============================] - 2186s 495ms/step - loss: 0.0562 - val_loss: 0.0754\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6d402d6650>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!rm -rf general_attention_encoder_decoder_tensorboard_logs\n",
        "# validation_data=([test_encoder_padded, test_decoder_padded_ip], test_decoder_padded_op)\n",
        "model.fit([train_encoder_padded, train_decoder_padded_ip], train_decoder_padded_op, epochs=10, batch_size=64,\n",
        "          validation_data=([test_encoder_padded, test_decoder_padded_ip], test_decoder_padded_op),\n",
        "          callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General-bsaed attention:\n",
        "\n",
        "\n",
        "\n",
        "1.   It is the multiplication between encoder output state and output of fully connected (Dense) layer of current decoder state.\n",
        "2.   The shape of encoder output states is (batch_size, input_length, lstm_units).\n",
        "3.   The shape of the decoder's current state is (batch_size, lstm_units).\n",
        "4.   We add another dimension at axis=1 in the current decoder state to make the multiplication product easier.\n",
        "5.   The softmax function passes the Multiplicative output to get attention weights.\n",
        "6.   A context vector is calculated by the Sum Of Product (SOP) between encoder output states and attention weights.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OAC5_6z1DMaq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TLwEo0TbCkHl",
        "outputId": "179a5593-abcc-483c-aaf1-00c14dad9c2c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "predicted = \" \".join(predict(test_encoder_padded[0], visualize_attention=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1ZILzUyHCpDD"
      },
      "outputs": [],
      "source": [
        "np.random.seed(101)\n",
        "indexes = np.random.permutation(1000)\n",
        "predicted_thousand_sentences = [\" \".join(predict(test_encoder_padded[index], visualize_attention=False)) for index in indexes]\n",
        "original_thousand_sentences = [X_test['english'].iloc[index] for index in indexes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nzXfnoNrCwOB",
        "outputId": "4652a453-bd55-4b96-923f-ef830ccb1f8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8407918975748012\n"
          ]
        }
      ],
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "avg_bleu = sum([bleu.sentence_bleu(reference, translated) for reference, translated in zip(original_thousand_sentences, predicted_thousand_sentences)]) / 1000\n",
        "print(avg_bleu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB1jRUqZQ9AM"
      },
      "source": [
        "<font color='blue'>**Repeat the same steps for Concat scoring function**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1110FSUjCzyz"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "logdir = os.path.join(\"concat_attention_encoder_decoder_tensorboard_logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQp5393nC6bp"
      },
      "outputs": [],
      "source": [
        "params = dict()\n",
        "params['input_vocab_size'] = vocab_size_encoder\n",
        "params['encoder_embedding_dim'] = 100\n",
        "params['lstm_size'] = 64\n",
        "params['encoder_input_length'] = 20\n",
        "params['scoring_function'] = 'concat'\n",
        "params['attention_units'] = 64\n",
        "\n",
        "params['output_vocab_size'] = vocab_size_decoder_ip\n",
        "params['decoder_embedding_dim'] = 100\n",
        "params['decoder_input_length'] = 20\n",
        "\n",
        "model = encoder_decoder(params)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=loss_function, run_eagerly=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYrs1veVDA43",
        "outputId": "87454edc-fe97-4744-918d-b9db261ac3a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4416/4416 [==============================] - 2559s 547ms/step - loss: 0.3178 - val_loss: 0.2529\n",
            "Epoch 2/10\n",
            "4416/4416 [==============================] - 2410s 546ms/step - loss: 0.2231 - val_loss: 0.1944\n",
            "Epoch 3/10\n",
            "4416/4416 [==============================] - 2378s 538ms/step - loss: 0.1698 - val_loss: 0.1508\n",
            "Epoch 4/10\n",
            "4416/4416 [==============================] - 2402s 544ms/step - loss: 0.1302 - val_loss: 0.1212\n",
            "Epoch 5/10\n",
            "4416/4416 [==============================] - 2383s 540ms/step - loss: 0.1036 - val_loss: 0.1035\n",
            "Epoch 6/10\n",
            "4416/4416 [==============================] - 2424s 549ms/step - loss: 0.0857 - val_loss: 0.0914\n",
            "Epoch 7/10\n",
            "4416/4416 [==============================] - 2378s 538ms/step - loss: 0.0734 - val_loss: 0.0830\n",
            "Epoch 8/10\n",
            "4416/4416 [==============================] - 2372s 537ms/step - loss: 0.0644 - val_loss: 0.0777\n",
            "Epoch 9/10\n",
            "4416/4416 [==============================] - 2402s 544ms/step - loss: 0.0576 - val_loss: 0.0734\n",
            "Epoch 10/10\n",
            "4416/4416 [==============================] - 2361s 535ms/step - loss: 0.0522 - val_loss: 0.0711\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0fa8615850>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!rm -rf concat_attention_encoder_decoder_tensorboard_logs\n",
        "model.fit([train_encoder_padded, train_decoder_padded_ip], train_decoder_padded_op, epochs=10, batch_size=64,\n",
        "          validation_data=([test_encoder_padded, test_decoder_padded_ip], test_decoder_padded_op),\n",
        "          callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concat-based attention:\n",
        "\n",
        "\n",
        "\n",
        "1.   The summation of output between two dense layers (encoder output and current decoder state) is passed on to the tanh function, further passed via the third fully connected layer.\n",
        "2.   The shape of encoder output states is (batch_size, input_length, lstm_units).\n",
        "3.   The shape of the decoder's current state is (batch_size, lstm_units).\n",
        "4.   We add another dimension at axis=1 in the current decoder state to make the multiplication product easier.\n",
        "5.   The softmax function passes the fully connected output to get attention weights.\n",
        "6. A context vector is calculated by the Sum Of Product (SOP) between encoder output states and attention weights.\n",
        "\n"
      ],
      "metadata": {
        "id": "VC2WOCWgDq2P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJiOiDHTDEwC"
      },
      "outputs": [],
      "source": [
        "predicted = \" \".join(predict(test_encoder_padded[0], visualize_attention=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x7UdFO_JUZJ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(101)\n",
        "indexes = np.random.permutation(1000)\n",
        "predicted_thousand_sentences = [\" \".join(predict(test_encoder_padded[index], visualize_attention=False)) for index in indexes]\n",
        "original_thousand_sentences = [X_test['english'].iloc[index] for index in indexes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kN9ZWViQNMB"
      },
      "outputs": [],
      "source": [
        "#Compile and train your model on concat scoring function.\n",
        "# Visualize few sentences randomly in Test data\n",
        "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
        "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
        "\n",
        "import nltk.translate.bleu_score as bleu\n",
        "avg_bleu = sum([bleu.sentence_bleu(reference, translated) for reference, translated in zip(original_thousand_sentences, predicted_thousand_sentences)]) / 1000\n",
        "print(avg_bleu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff1lV0ITM6_p"
      },
      "outputs": [],
      "source": [
        "# Write your observations on each of the scoring function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the training observation, it looks like concat based attention performs better than comparing to general and dot-based. There is an apparent reason for that: Concat has more learning parameters, as it has a more dense layer. "
      ],
      "metadata": {
        "id": "25dtXL2oFWvR"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Seq2SeqImplementation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}